{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94871e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Loading FastText model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FastText model loaded successfully!\n",
      "\n",
      "====================================================================================================\n",
      "Evaluating summaries for the first 30 documents\n",
      "====================================================================================================\n",
      "\n",
      "Similarity method: cosine\n",
      "Generating and evaluating summaries using cosine similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores:\n",
      "ROUGE-1 -> Precision: 0.1758, Recall: 0.4259, F1-Score: 0.2408\n",
      "ROUGE-2 -> Precision: 0.0550, Recall: 0.1766, F1-Score: 0.0835\n",
      "ROUGE-L -> Precision: 0.1451, Recall: 0.3446, F1-Score: 0.1969\n",
      "\n",
      "Similarity method: euclidean\n",
      "Generating and evaluating summaries using euclidean similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores:\n",
      "ROUGE-1 -> Precision: 0.1780, Recall: 0.4372, F1-Score: 0.2459\n",
      "ROUGE-2 -> Precision: 0.0560, Recall: 0.1755, F1-Score: 0.0846\n",
      "ROUGE-L -> Precision: 0.1469, Recall: 0.3541, F1-Score: 0.2010\n",
      "\n",
      "Similarity method: manhattan\n",
      "Generating and evaluating summaries using manhattan similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores:\n",
      "ROUGE-1 -> Precision: 0.1818, Recall: 0.4517, F1-Score: 0.2520\n",
      "ROUGE-2 -> Precision: 0.0580, Recall: 0.1855, F1-Score: 0.0879\n",
      "ROUGE-L -> Precision: 0.1520, Recall: 0.3727, F1-Score: 0.2091\n",
      "\n",
      "====================================================================================================\n",
      "Evaluating summaries for the first 50 documents\n",
      "====================================================================================================\n",
      "\n",
      "Similarity method: cosine\n",
      "Generating and evaluating summaries using cosine similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores:\n",
      "ROUGE-1 -> Precision: 0.1714, Recall: 0.4163, F1-Score: 0.2356\n",
      "ROUGE-2 -> Precision: 0.0480, Recall: 0.1479, F1-Score: 0.0717\n",
      "ROUGE-L -> Precision: 0.1375, Recall: 0.3303, F1-Score: 0.1877\n",
      "\n",
      "Similarity method: euclidean\n",
      "Generating and evaluating summaries using euclidean similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores:\n",
      "ROUGE-1 -> Precision: 0.1717, Recall: 0.4218, F1-Score: 0.2379\n",
      "ROUGE-2 -> Precision: 0.0471, Recall: 0.1423, F1-Score: 0.0703\n",
      "ROUGE-L -> Precision: 0.1381, Recall: 0.3331, F1-Score: 0.1897\n",
      "\n",
      "Similarity method: manhattan\n",
      "Generating and evaluating summaries using manhattan similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores:\n",
      "ROUGE-1 -> Precision: 0.1742, Recall: 0.4310, F1-Score: 0.2421\n",
      "ROUGE-2 -> Precision: 0.0480, Recall: 0.1502, F1-Score: 0.0722\n",
      "ROUGE-L -> Precision: 0.1414, Recall: 0.3467, F1-Score: 0.1953\n",
      "\n",
      "====================================================================================================\n",
      "Evaluating summaries for the first 100 documents\n",
      "====================================================================================================\n",
      "\n",
      "Similarity method: cosine\n",
      "Generating and evaluating summaries using cosine similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores:\n",
      "ROUGE-1 -> Precision: 0.2028, Recall: 0.4704, F1-Score: 0.2729\n",
      "ROUGE-2 -> Precision: 0.0771, Recall: 0.2326, F1-Score: 0.1119\n",
      "ROUGE-L -> Precision: 0.1707, Recall: 0.3938, F1-Score: 0.2284\n",
      "\n",
      "Similarity method: euclidean\n",
      "Generating and evaluating summaries using euclidean similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores:\n",
      "ROUGE-1 -> Precision: 0.2090, Recall: 0.4925, F1-Score: 0.2841\n",
      "ROUGE-2 -> Precision: 0.0815, Recall: 0.2432, F1-Score: 0.1186\n",
      "ROUGE-L -> Precision: 0.1727, Recall: 0.4038, F1-Score: 0.2332\n",
      "\n",
      "Similarity method: manhattan\n",
      "Generating and evaluating summaries using manhattan similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores:\n",
      "ROUGE-1 -> Precision: 0.2110, Recall: 0.4896, F1-Score: 0.2853\n",
      "ROUGE-2 -> Precision: 0.0828, Recall: 0.2442, F1-Score: 0.1199\n",
      "ROUGE-L -> Precision: 0.1811, Recall: 0.4168, F1-Score: 0.2434\n",
      "\n",
      "====================================================================================================\n",
      "Evaluating summaries for the first 200 documents\n",
      "====================================================================================================\n",
      "\n",
      "Similarity method: cosine\n",
      "Generating and evaluating summaries using cosine similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores:\n",
      "ROUGE-1 -> Precision: 0.2401, Recall: 0.5370, F1-Score: 0.3213\n",
      "ROUGE-2 -> Precision: 0.1157, Recall: 0.3338, F1-Score: 0.1675\n",
      "ROUGE-L -> Precision: 0.2080, Recall: 0.4634, F1-Score: 0.2774\n",
      "\n",
      "Similarity method: euclidean\n",
      "Generating and evaluating summaries using euclidean similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores:\n",
      "ROUGE-1 -> Precision: 0.2440, Recall: 0.5464, F1-Score: 0.3270\n",
      "ROUGE-2 -> Precision: 0.1204, Recall: 0.3409, F1-Score: 0.1734\n",
      "ROUGE-L -> Precision: 0.2122, Recall: 0.4715, F1-Score: 0.2832\n",
      "\n",
      "Similarity method: manhattan\n",
      "Generating and evaluating summaries using manhattan similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores:\n",
      "ROUGE-1 -> Precision: 0.2441, Recall: 0.5441, F1-Score: 0.3265\n",
      "ROUGE-2 -> Precision: 0.1187, Recall: 0.3326, F1-Score: 0.1703\n",
      "ROUGE-L -> Precision: 0.2152, Recall: 0.4759, F1-Score: 0.2866\n",
      "\n",
      "====================================================================================================\n",
      "Evaluating summaries for the first 500 documents\n",
      "====================================================================================================\n",
      "\n",
      "Similarity method: cosine\n",
      "Generating and evaluating summaries using cosine similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores:\n",
      "ROUGE-1 -> Precision: 0.2340, Recall: 0.5335, F1-Score: 0.3156\n",
      "ROUGE-2 -> Precision: 0.1159, Recall: 0.3323, F1-Score: 0.1679\n",
      "ROUGE-L -> Precision: 0.2006, Recall: 0.4560, F1-Score: 0.2701\n",
      "\n",
      "Similarity method: euclidean\n",
      "Generating and evaluating summaries using euclidean similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores:\n",
      "ROUGE-1 -> Precision: 0.2360, Recall: 0.5379, F1-Score: 0.3186\n",
      "ROUGE-2 -> Precision: 0.1180, Recall: 0.3324, F1-Score: 0.1702\n",
      "ROUGE-L -> Precision: 0.2021, Recall: 0.4588, F1-Score: 0.2721\n",
      "\n",
      "Similarity method: manhattan\n",
      "Generating and evaluating summaries using manhattan similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores:\n",
      "ROUGE-1 -> Precision: 0.2373, Recall: 0.5376, F1-Score: 0.3196\n",
      "ROUGE-2 -> Precision: 0.1185, Recall: 0.3289, F1-Score: 0.1701\n",
      "ROUGE-L -> Precision: 0.2057, Recall: 0.4636, F1-Score: 0.2763\n",
      "\n",
      "Saved average ROUGE scores to 'rouge_scores_summary.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rouge import Rouge\n",
    "from scipy.spatial import distance\n",
    "import fasttext.util\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Load FastText model for Hindi\n",
    "print(\"⏳ Loading FastText model...\")\n",
    "fasttext.util.download_model('hi', if_exists='ignore')\n",
    "model = fasttext.load_model('cc.hi.300.bin')\n",
    "print(\"✅ FastText model loaded successfully!\")\n",
    "\n",
    "class HindiTextSummarizer:\n",
    "    def __init__(self, summary_length=150):  # Increased default length\n",
    "        self.summary_length = summary_length\n",
    "        self.rouge = Rouge()\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        if pd.isna(text):\n",
    "            return []\n",
    "        \n",
    "        text = str(text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        sentences = re.split(r'[।!?.]', text)\n",
    "        \n",
    "        cleaned_sentences = []\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if len(sentence.split()) < 2:  # Reduced from 3 to 2 words minimum\n",
    "                continue\n",
    "            try:\n",
    "                words = word_tokenize(sentence, language=\"hindi\")\n",
    "            except:\n",
    "                words = sentence.split()\n",
    "            cleaned_sentence = \" \".join(words)\n",
    "            if cleaned_sentence and cleaned_sentence not in cleaned_sentences:\n",
    "                cleaned_sentences.append(cleaned_sentence)\n",
    "        \n",
    "        return cleaned_sentences\n",
    "\n",
    "    def jaccard_similarity(self, s1, s2):\n",
    "        \"\"\"Jaccard similarity for duplicate detection\"\"\"\n",
    "        words1 = set(s1.split())\n",
    "        words2 = set(s2.split())\n",
    "        intersection = len(words1.intersection(words2))\n",
    "        union = len(words1.union(words2))\n",
    "        return intersection / union if union != 0 else 0\n",
    "\n",
    "    def sentence_to_vector(self, sentence):\n",
    "        \"\"\"Convert sentence to vector using FastText word embeddings\"\"\"\n",
    "        words = sentence.split()\n",
    "        word_vectors = []\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                word_vectors.append(model[word])\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if len(word_vectors) == 0:\n",
    "            return np.zeros(model.get_dimension())\n",
    "        \n",
    "        return np.mean(word_vectors, axis=0)\n",
    "\n",
    "    def cosine_similarity_ft(self, s1, s2):\n",
    "        \"\"\"Cosine similarity using FastText vectors\"\"\"\n",
    "        vec1 = self.sentence_to_vector(s1)\n",
    "        vec2 = self.sentence_to_vector(s2)\n",
    "        \n",
    "        if np.allclose(vec1, 0) or np.allclose(vec2, 0):\n",
    "            return 0.0\n",
    "        \n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "        \n",
    "        if norm_product == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return max(0.0, dot_product / norm_product)\n",
    "\n",
    "    def euclidean_similarity_ft(self, s1, s2):\n",
    "        \"\"\"Convert Euclidean distance to similarity\"\"\"\n",
    "        vec1 = self.sentence_to_vector(s1)\n",
    "        vec2 = self.sentence_to_vector(s2)\n",
    "        \n",
    "        if np.allclose(vec1, 0) or np.allclose(vec2, 0):\n",
    "            return 0.0\n",
    "        \n",
    "        dist = distance.euclidean(vec1, vec2)\n",
    "        # Less aggressive conversion - increased divisor\n",
    "        return np.exp(-dist / 15.0)  # Increased from 5.0 to 15.0\n",
    "\n",
    "    def manhattan_similarity_ft(self, s1, s2):\n",
    "        \"\"\"Convert Manhattan distance to similarity\"\"\"\n",
    "        vec1 = self.sentence_to_vector(s1)\n",
    "        vec2 = self.sentence_to_vector(s2)\n",
    "        \n",
    "        if np.allclose(vec1, 0) or np.allclose(vec2, 0):\n",
    "            return 0.0\n",
    "        \n",
    "        dist = distance.cityblock(vec1, vec2)\n",
    "        return np.exp(-dist / 150.0)  # Increased from 50.0 to 150.0\n",
    "\n",
    "    def build_similarity_matrix_ft(self, sentences, similarity_measure):\n",
    "        \"\"\"Build similarity matrix for FastText-based methods\"\"\"\n",
    "        num_sentences = len(sentences)\n",
    "        similarity_matrix = np.zeros((num_sentences, num_sentences))\n",
    "        \n",
    "        for i in range(num_sentences):\n",
    "            for j in range(num_sentences):\n",
    "                if i == j:\n",
    "                    similarity_matrix[i][j] = 1.0  # Restore self-similarity\n",
    "                else:\n",
    "                    if similarity_measure == \"cosine\":\n",
    "                        sim = self.cosine_similarity_ft(sentences[i], sentences[j])\n",
    "                    elif similarity_measure == \"euclidean\":\n",
    "                        sim = self.euclidean_similarity_ft(sentences[i], sentences[j])\n",
    "                    elif similarity_measure == \"manhattan\":\n",
    "                        sim = self.manhattan_similarity_ft(sentences[i], sentences[j])\n",
    "                    else:\n",
    "                        sim = self.cosine_similarity_ft(sentences[i], sentences[j])\n",
    "                    \n",
    "                    similarity_matrix[i][j] = sim\n",
    "\n",
    "        return similarity_matrix\n",
    "\n",
    "    def rank_sentences(self, sentences, similarity_matrix):\n",
    "        \"\"\"Rank sentences using PageRank algorithm\"\"\"\n",
    "        try:\n",
    "            # Add small epsilon to avoid zero connectivity\n",
    "            similarity_matrix = similarity_matrix + 1e-8\n",
    "            \n",
    "            graph = nx.from_numpy_array(similarity_matrix)\n",
    "            scores = nx.pagerank(graph, alpha=0.85, max_iter=100, tol=1e-6)\n",
    "            return sorted(((scores[i], sentence) for i, sentence in enumerate(sentences)), reverse=True)\n",
    "        except:\n",
    "            # Fallback: rank by sum of similarities\n",
    "            scores = np.sum(similarity_matrix, axis=1)\n",
    "            return sorted(((scores[i], sentence) for i, sentence in enumerate(sentences)), reverse=True)\n",
    "\n",
    "    def generate_summary_ft(self, text, similarity_measure=\"cosine\"):\n",
    "        \"\"\"Generate summary using FastText embeddings with balanced length control\"\"\"\n",
    "        sentences = self.preprocess_text(text)\n",
    "        if not sentences:\n",
    "            return \"\"\n",
    "        \n",
    "        # Process more sentences but still limit for efficiency\n",
    "        if len(sentences) > 30:\n",
    "            sentences = sentences[:30]  # Increased from 20 to 30\n",
    "        \n",
    "        similarity_matrix = self.build_similarity_matrix_ft(sentences, similarity_measure)\n",
    "        ranked_sentences = self.rank_sentences(sentences, similarity_matrix)\n",
    "        \n",
    "        summary = []\n",
    "        selected_sentences = set()\n",
    "        word_count = 0\n",
    "        \n",
    "        for score, sentence in ranked_sentences:\n",
    "            # More lenient duplicate detection\n",
    "            if any(self.jaccard_similarity(sentence, s) > 0.8 for s in selected_sentences):\n",
    "                continue\n",
    "            \n",
    "            sentence_word_count = len(sentence.split())\n",
    "            \n",
    "            # More flexible word count with buffer\n",
    "            if word_count + sentence_word_count <= self.summary_length:\n",
    "                summary.append(sentence)\n",
    "                selected_sentences.add(sentence)\n",
    "                word_count += sentence_word_count\n",
    "            elif word_count < self.summary_length * 0.7:  # If we haven't reached 70% of target\n",
    "                # Allow slightly longer summaries to improve recall\n",
    "                remaining_words = int(self.summary_length * 1.2) - word_count  # 20% buffer\n",
    "                if sentence_word_count <= remaining_words:\n",
    "                    summary.append(sentence)\n",
    "                    selected_sentences.add(sentence)\n",
    "                    word_count += sentence_word_count\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return \" \".join(summary)\n",
    "\n",
    "    def evaluate_summary(self, generated_summary, reference_summary):\n",
    "        \"\"\"Evaluate summary using ROUGE metrics\"\"\"\n",
    "        if pd.isna(generated_summary) or pd.isna(reference_summary):\n",
    "            return None\n",
    "        \n",
    "        generated_summary = str(generated_summary).strip()\n",
    "        reference_summary = str(reference_summary).strip()\n",
    "        \n",
    "        if not generated_summary or not reference_summary:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            scores = self.rouge.get_scores(generated_summary, reference_summary)\n",
    "            return scores\n",
    "        except Exception as e:\n",
    "            print(f\"ROUGE evaluation error: {e}\")\n",
    "            return None\n",
    "\n",
    "from tqdm import tqdm  # Add this import at the top\n",
    "\n",
    "def compute_average_rouge(summarizer, df_subset, similarity_measure):\n",
    "    rouge_scores = []\n",
    "\n",
    "    print(f\"Generating and evaluating summaries using {similarity_measure} similarity...\")\n",
    "    for _, row in tqdm(df_subset.iterrows(), total=len(df_subset), desc=\"Progress\", leave=False):\n",
    "        article = row[\"article\"]\n",
    "        reference = row[\"summary\"]\n",
    "\n",
    "        generated_summary = summarizer.generate_summary_ft(article, similarity_measure=similarity_measure)\n",
    "        scores = summarizer.evaluate_summary(generated_summary, reference)\n",
    "        if scores:\n",
    "            rouge_scores.append(scores[0])\n",
    "\n",
    "    if not rouge_scores:\n",
    "        return None\n",
    "\n",
    "    # Compute average scores\n",
    "    avg_scores = {\n",
    "        \"rouge-1\": {\n",
    "            \"p\": np.mean([score[\"rouge-1\"][\"p\"] for score in rouge_scores]),\n",
    "            \"r\": np.mean([score[\"rouge-1\"][\"r\"] for score in rouge_scores]),\n",
    "            \"f\": np.mean([score[\"rouge-1\"][\"f\"] for score in rouge_scores])\n",
    "        },\n",
    "        \"rouge-2\": {\n",
    "            \"p\": np.mean([score[\"rouge-2\"][\"p\"] for score in rouge_scores]),\n",
    "            \"r\": np.mean([score[\"rouge-2\"][\"r\"] for score in rouge_scores]),\n",
    "            \"f\": np.mean([score[\"rouge-2\"][\"f\"] for score in rouge_scores])\n",
    "        },\n",
    "        \"rouge-l\": {\n",
    "            \"p\": np.mean([score[\"rouge-l\"][\"p\"] for score in rouge_scores]),\n",
    "            \"r\": np.mean([score[\"rouge-l\"][\"r\"] for score in rouge_scores]),\n",
    "            \"f\": np.mean([score[\"rouge-l\"][\"f\"] for score in rouge_scores])\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return avg_scores\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        hindi_data = pd.read_csv(\"test.csv\", encoding=\"utf-8\", nrows=600)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Dataset not found.\")\n",
    "        return\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: The CSV file is empty.\")\n",
    "        return\n",
    "\n",
    "    subset_sizes = [30, 50, 100, 200, 500]\n",
    "    summary_length = 100\n",
    "    fasttext_measures = [\"cosine\", \"euclidean\", \"manhattan\"]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for size in subset_sizes:\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"Evaluating summaries for the first {size} documents\")\n",
    "        print(\"=\" * 100)\n",
    "\n",
    "        df_subset = hindi_data.iloc[:size].dropna(subset=[\"article\", \"summary\"])\n",
    "        summarizer = HindiTextSummarizer(summary_length=summary_length)\n",
    "\n",
    "        for measure in fasttext_measures:\n",
    "            print(f\"\\nSimilarity method: {measure}\")\n",
    "            avg_scores = compute_average_rouge(summarizer, df_subset, measure)\n",
    "\n",
    "            if avg_scores:\n",
    "                print(\"Average ROUGE Scores:\")\n",
    "                for metric in [\"rouge-1\", \"rouge-2\", \"rouge-l\"]:\n",
    "                    score = avg_scores[metric]\n",
    "                    print(f\"{metric.upper()} -> Precision: {score['p']:.4f}, Recall: {score['r']:.4f}, F1-Score: {score['f']:.4f}\")\n",
    "\n",
    "                # Add row to results\n",
    "                results.append({\n",
    "                    \"Subset Size\": size,\n",
    "                    \"Similarity\": measure,\n",
    "                    \"ROUGE-1 Precision\": avg_scores[\"rouge-1\"][\"p\"],\n",
    "                    \"ROUGE-1 Recall\": avg_scores[\"rouge-1\"][\"r\"],\n",
    "                    \"ROUGE-1 F1\": avg_scores[\"rouge-1\"][\"f\"],\n",
    "                    \"ROUGE-2 Precision\": avg_scores[\"rouge-2\"][\"p\"],\n",
    "                    \"ROUGE-2 Recall\": avg_scores[\"rouge-2\"][\"r\"],\n",
    "                    \"ROUGE-2 F1\": avg_scores[\"rouge-2\"][\"f\"],\n",
    "                    \"ROUGE-L Precision\": avg_scores[\"rouge-l\"][\"p\"],\n",
    "                    \"ROUGE-L Recall\": avg_scores[\"rouge-l\"][\"r\"],\n",
    "                    \"ROUGE-L F1\": avg_scores[\"rouge-l\"][\"f\"]\n",
    "                })\n",
    "            else:\n",
    "                print(\"ROUGE score computation failed. This may be due to empty or invalid summaries.\")\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(\"rouge_scores_summary.csv\", index=False)\n",
    "    print(\"\\nSaved average ROUGE scores to 'rouge_scores_summary.csv'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd1812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
