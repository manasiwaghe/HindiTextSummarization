{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05:16:52:50,989 INFO     [iwn.py:43] Loading hindi language synsets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Evaluating on 30 documents...\n",
      "  ğŸ”¹ Processed 10/30 documents...\n",
      "  ğŸ”¹ Processed 20/30 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05:17:36:12,589 INFO     [iwn.py:43] Loading hindi language synsets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ”¹ Processed 30/30 documents...\n",
      "âœ… ROUGE-1 â†’ P: 0.2314, R: 0.6084, F1: 0.3319\n",
      "âœ… ROUGE-2 â†’ P: 0.1035, R: 0.3240, F1: 0.1551\n",
      "âœ… ROUGE-L â†’ P: 0.1900, R: 0.5012, F1: 0.2727\n",
      "\n",
      "ğŸ“„ Evaluating on 50 documents...\n",
      "  ğŸ”¹ Processed 10/50 documents...\n",
      "  ğŸ”¹ Processed 20/50 documents...\n",
      "  ğŸ”¹ Processed 30/50 documents...\n",
      "  ğŸ”¹ Processed 40/50 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05:18:27:25,622 INFO     [iwn.py:43] Loading hindi language synsets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ”¹ Processed 50/50 documents...\n",
      "âœ… ROUGE-1 â†’ P: 0.2551, R: 0.6352, F1: 0.3583\n",
      "âœ… ROUGE-2 â†’ P: 0.1315, R: 0.3698, F1: 0.1901\n",
      "âœ… ROUGE-L â†’ P: 0.2150, R: 0.5290, F1: 0.3008\n",
      "\n",
      "ğŸ“„ Evaluating on 100 documents...\n",
      "  ğŸ”¹ Processed 10/63 documents...\n",
      "  ğŸ”¹ Processed 20/63 documents...\n",
      "  ğŸ”¹ Processed 30/63 documents...\n",
      "  ğŸ”¹ Processed 40/63 documents...\n",
      "  ğŸ”¹ Processed 50/63 documents...\n",
      "  ğŸ”¹ Processed 60/63 documents...\n",
      "âœ… ROUGE-1 â†’ P: 0.2707, R: 0.6436, F1: 0.3737\n",
      "âœ… ROUGE-2 â†’ P: 0.1442, R: 0.3965, F1: 0.2059\n",
      "âœ… ROUGE-L â†’ P: 0.2274, R: 0.5398, F1: 0.3140\n",
      "\n",
      "ğŸ“ Saved scores to wordnet_rouge_scores.csv âœ…\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rouge import Rouge\n",
    "from pyiwn import IndoWordNet, Language\n",
    "from tqdm import tqdm  # âœ… Import tqdm for progress bars\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "class HindiTextSummarizer:\n",
    "    def __init__(self, summary_length=100):\n",
    "        self.summary_length = summary_length\n",
    "        self.rouge = Rouge()\n",
    "        self.iwn = IndoWordNet(lang=Language.HINDI)\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        if pd.isna(text):\n",
    "            return []\n",
    "\n",
    "        text = str(text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        sentences = re.split(r'[à¥¤!?.]', text)\n",
    "\n",
    "        cleaned_sentences = []\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if len(sentence.split()) < 3:\n",
    "                continue\n",
    "            try:\n",
    "                words = word_tokenize(sentence, language=\"hindi\")\n",
    "            except:\n",
    "                words = sentence.split()\n",
    "            cleaned_sentence = \" \".join(words)\n",
    "            if cleaned_sentence and cleaned_sentence not in cleaned_sentences:\n",
    "                cleaned_sentences.append(cleaned_sentence)\n",
    "\n",
    "        return cleaned_sentences\n",
    "\n",
    "    def wordnet_similarity(self, s1, s2):\n",
    "        words1 = set(s1.split())\n",
    "        words2 = set(s2.split())\n",
    "        similarity_score = 0\n",
    "\n",
    "        for word1 in words1:\n",
    "            for word2 in words2:\n",
    "                try:\n",
    "                    synsets1 = self.iwn.synsets(word1)\n",
    "                    synsets2 = self.iwn.synsets(word2)\n",
    "                    if synsets1 and synsets2:\n",
    "                        similarity_score += sum(1 for _ in set(synsets1) & set(synsets2))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        denominator = np.sqrt(len(words1)) * np.sqrt(len(words2))\n",
    "        return similarity_score / denominator if denominator != 0 else 0\n",
    "\n",
    "    def build_similarity_matrix(self, sentences):\n",
    "        num_sentences = len(sentences)\n",
    "        similarity_matrix = np.zeros((num_sentences, num_sentences))\n",
    "\n",
    "        for i in range(num_sentences):\n",
    "            for j in range(num_sentences):\n",
    "                if i != j:\n",
    "                    similarity_matrix[i][j] = self.wordnet_similarity(sentences[i], sentences[j])\n",
    "        return similarity_matrix\n",
    "\n",
    "    def rank_sentences(self, sentences, similarity_matrix):\n",
    "        graph = nx.from_numpy_array(similarity_matrix)\n",
    "        scores = nx.pagerank(graph, alpha=0.85)\n",
    "\n",
    "        sorted_sentences = sorted(\n",
    "            ((scores[i], sentence) for i, sentence in enumerate(sentences)),\n",
    "            reverse=True\n",
    "        )\n",
    "        return sorted_sentences\n",
    "\n",
    "    def generate_summary(self, text):\n",
    "        sentences = self.preprocess_text(text)\n",
    "        if not sentences:\n",
    "            return \"\"\n",
    "\n",
    "        similarity_matrix = self.build_similarity_matrix(sentences)\n",
    "        ranked_sentences = self.rank_sentences(sentences, similarity_matrix)\n",
    "\n",
    "        summary = []\n",
    "        word_count = 0\n",
    "        for score, sentence in ranked_sentences:\n",
    "            if all(self.wordnet_similarity(sentence, existing) < 0.7 for existing in summary):\n",
    "                if word_count + len(sentence.split()) <= self.summary_length:\n",
    "                    summary.append(sentence)\n",
    "                    word_count += len(sentence.split())\n",
    "            if word_count >= self.summary_length:\n",
    "                break\n",
    "\n",
    "        return \" \".join(summary)\n",
    "\n",
    "def compute_average_rouge(summarizer, df_subset):\n",
    "    rouge = Rouge()\n",
    "    rouge_1, rouge_2, rouge_l = [], [], []\n",
    "\n",
    "    for idx, row in enumerate(df_subset.itertuples(), start=1):\n",
    "        article = row.article\n",
    "        reference_summary = row.summary\n",
    "        generated_summary = summarizer.generate_summary(article)\n",
    "\n",
    "        if generated_summary.strip():\n",
    "            try:\n",
    "                scores = rouge.get_scores(generated_summary, reference_summary)[0]\n",
    "                rouge_1.append(scores[\"rouge-1\"])\n",
    "                rouge_2.append(scores[\"rouge-2\"])\n",
    "                rouge_l.append(scores[\"rouge-l\"])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"  ğŸ”¹ Processed {idx}/{len(df_subset)} documents...\")\n",
    "\n",
    "    def avg(metric_scores):\n",
    "        return {\n",
    "            \"p\": np.mean([s[\"p\"] for s in metric_scores]) if metric_scores else 0,\n",
    "            \"r\": np.mean([s[\"r\"] for s in metric_scores]) if metric_scores else 0,\n",
    "            \"f\": np.mean([s[\"f\"] for s in metric_scores]) if metric_scores else 0\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"rouge-1\": avg(rouge_1),\n",
    "        \"rouge-2\": avg(rouge_2),\n",
    "        \"rouge-l\": avg(rouge_l)\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        df = pd.read_csv(\"test.csv\", encoding=\"utf-8\", nrows=200)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error reading dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    df.dropna(subset=[\"summary\"], inplace=True)\n",
    "    df = df[df[\"article\"].str.split().str.len() >= 200]\n",
    "\n",
    "    subset_sizes = [30, 50, 100]\n",
    "    results = []\n",
    "\n",
    "    for size in subset_sizes:\n",
    "        print(f\"\\nğŸ“„ Evaluating on {size} documents...\")\n",
    "        summarizer = HindiTextSummarizer(summary_length=100)\n",
    "        df_subset = df.iloc[:size]\n",
    "        avg_scores = compute_average_rouge(summarizer, df_subset)\n",
    "\n",
    "        for metric in [\"rouge-1\", \"rouge-2\", \"rouge-l\"]:\n",
    "            score = avg_scores[metric]\n",
    "            print(f\"âœ… {metric.upper()} â†’ P: {score['p']:.4f}, R: {score['r']:.4f}, F1: {score['f']:.4f}\")\n",
    "\n",
    "        results.append({\n",
    "            \"Subset Size\": size,\n",
    "            \"ROUGE-1 Precision\": avg_scores[\"rouge-1\"][\"p\"],\n",
    "            \"ROUGE-1 Recall\": avg_scores[\"rouge-1\"][\"r\"],\n",
    "            \"ROUGE-1 F1\": avg_scores[\"rouge-1\"][\"f\"],\n",
    "            \"ROUGE-2 Precision\": avg_scores[\"rouge-2\"][\"p\"],\n",
    "            \"ROUGE-2 Recall\": avg_scores[\"rouge-2\"][\"r\"],\n",
    "            \"ROUGE-2 F1\": avg_scores[\"rouge-2\"][\"f\"],\n",
    "            \"ROUGE-L Precision\": avg_scores[\"rouge-l\"][\"p\"],\n",
    "            \"ROUGE-L Recall\": avg_scores[\"rouge-l\"][\"r\"],\n",
    "            \"ROUGE-L F1\": avg_scores[\"rouge-l\"][\"f\"]\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(\"wordnet_rouge_scores.csv\", index=False)\n",
    "    print(\"\\nğŸ“ Saved scores to wordnet_rouge_scores.csv âœ…\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
