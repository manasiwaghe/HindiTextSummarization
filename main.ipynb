{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94871e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 100 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Docs: 100: 100%|██████████| 100/100 [00:28<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 200 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Docs: 200: 100%|██████████| 100/100 [01:06<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 300 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Docs: 300: 100%|██████████| 100/100 [01:27<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 500 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Docs: 500: 100%|██████████| 100/100 [01:25<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compilation Table of Average ROUGE Scores (Precision, Recall, F1):\n",
      "\n",
      " Document Count Similarity Measure  ROUGE-1 Precision  ROUGE-1 Recall  ROUGE-1 F1  ROUGE-2 Precision  ROUGE-2 Recall  ROUGE-2 F1  ROUGE-L Precision  ROUGE-L Recall  ROUGE-L F1\n",
      "            100      common_tokens              0.272           0.594       0.368              0.141           0.351       0.198              0.228           0.493       0.307\n",
      "            100             cosine              0.278           0.601       0.375              0.142           0.366       0.202              0.233           0.504       0.315\n",
      "            100            jaccard              0.266           0.573       0.357              0.128           0.326       0.180              0.219           0.470       0.294\n",
      "            200      common_tokens              0.272           0.594       0.368              0.141           0.351       0.198              0.228           0.493       0.307\n",
      "            200             cosine              0.278           0.601       0.375              0.142           0.366       0.202              0.233           0.504       0.315\n",
      "            200            jaccard              0.266           0.573       0.357              0.128           0.326       0.180              0.219           0.470       0.294\n",
      "            300      common_tokens              0.272           0.594       0.368              0.141           0.351       0.198              0.228           0.493       0.307\n",
      "            300             cosine              0.278           0.601       0.375              0.142           0.366       0.202              0.233           0.504       0.315\n",
      "            300            jaccard              0.266           0.573       0.357              0.128           0.326       0.180              0.219           0.470       0.294\n",
      "            500      common_tokens              0.272           0.594       0.368              0.141           0.351       0.198              0.228           0.493       0.307\n",
      "            500             cosine              0.278           0.601       0.375              0.142           0.366       0.202              0.233           0.504       0.315\n",
      "            500            jaccard              0.266           0.573       0.357              0.128           0.326       0.180              0.219           0.470       0.294\n",
      "\n",
      "Detailed results exported to compiled_summary_rouge_scores_detailed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import re\n",
    "import chardet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "class HindiTextSummarizer:\n",
    "    def __init__(self, summary_length=300):\n",
    "        self.summary_length = summary_length\n",
    "        self.rouge = Rouge()\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        if pd.isna(text):\n",
    "            return []\n",
    "        \n",
    "        text = str(text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        sentences = re.split(r'[।!?.]', text)\n",
    "        \n",
    "        cleaned_sentences = []\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if len(sentence.split()) < 3:\n",
    "                continue\n",
    "            try:\n",
    "                words = word_tokenize(sentence, language=\"hindi\")\n",
    "            except:\n",
    "                words = sentence.split()\n",
    "            cleaned_sentence = \" \".join(words)\n",
    "            if cleaned_sentence and cleaned_sentence not in cleaned_sentences:\n",
    "                cleaned_sentences.append(cleaned_sentence)\n",
    "        \n",
    "        return cleaned_sentences\n",
    "\n",
    "    def build_similarity_matrix(self, sentences, similarity_measure=\"common_tokens\"):\n",
    "        num_sentences = len(sentences)\n",
    "        similarity_matrix = np.zeros((num_sentences, num_sentences))\n",
    "        \n",
    "        for i in range(num_sentences):\n",
    "            for j in range(num_sentences):\n",
    "                if i != j and i <= j:\n",
    "                    if similarity_measure == \"common_tokens\":\n",
    "                        similarity_matrix[i][j] = self.common_tokens_similarity(sentences[i], sentences[j])\n",
    "                    elif similarity_measure == \"cosine\":\n",
    "                        similarity_matrix[i][j] = self.cosine_similarity(sentences[i], sentences[j])\n",
    "                    elif similarity_measure == \"jaccard\":\n",
    "                        similarity_matrix[i][j] = self.jaccard_similarity(sentences[i], sentences[j])\n",
    "        \n",
    "        return similarity_matrix\n",
    "\n",
    "    def common_tokens_similarity(self, s1, s2):\n",
    "        words1 = set(s1.split())\n",
    "        words2 = set(s2.split())\n",
    "        common_tokens = len(words1.intersection(words2))\n",
    "        denominator = np.log(len(words1) + 1) + np.log(len(words2) + 1)\n",
    "        return common_tokens / denominator if denominator != 0 else 0\n",
    "\n",
    "    def cosine_similarity(self, s1, s2):\n",
    "        words1 = set(s1.split())\n",
    "        words2 = set(s2.split())\n",
    "        intersection = len(words1.intersection(words2))\n",
    "        denominator = np.sqrt(len(words1)) * np.sqrt(len(words2))\n",
    "        return intersection / denominator if denominator != 0 else 0\n",
    "\n",
    "    def jaccard_similarity(self, s1, s2):\n",
    "        words1 = set(s1.split())\n",
    "        words2 = set(s2.split())\n",
    "        intersection = len(words1.intersection(words2))\n",
    "        union = len(words1.union(words2))\n",
    "        return intersection / union if union != 0 else 0\n",
    "\n",
    "    def rank_sentences(self, sentences, similarity_matrix):\n",
    "        graph = nx.from_numpy_array(similarity_matrix)\n",
    "        scores = nx.pagerank(graph, alpha=0.85)\n",
    "        return sorted(((scores[i], sentence) for i, sentence in enumerate(sentences)), reverse=True)\n",
    "\n",
    "    def generate_summary(self, text, similarity_measure=\"common_tokens\"):\n",
    "        sentences = self.preprocess_text(text)\n",
    "        if not sentences:\n",
    "            return \"\"\n",
    "        similarity_matrix = self.build_similarity_matrix(sentences, similarity_measure)\n",
    "        ranked_sentences = self.rank_sentences(sentences, similarity_matrix)\n",
    "        summary = []\n",
    "        selected_sentences = set()\n",
    "        word_count = 0\n",
    "        for score, sentence in ranked_sentences:\n",
    "            if any(self.jaccard_similarity(sentence, s) > 0.7 for s in selected_sentences):\n",
    "                continue\n",
    "            if word_count + len(sentence.split()) <= self.summary_length:\n",
    "                summary.append(sentence)\n",
    "                selected_sentences.add(sentence)\n",
    "                word_count += len(sentence.split())\n",
    "            else:\n",
    "                break\n",
    "        return \" \".join(summary)\n",
    "\n",
    "    def evaluate_summary(self, generated_summary, reference_summary):\n",
    "        if pd.isna(generated_summary) or pd.isna(reference_summary):\n",
    "            return None\n",
    "        generated_summary = str(generated_summary).strip()\n",
    "        reference_summary = str(reference_summary).strip()\n",
    "        if not generated_summary or not reference_summary:\n",
    "            return None\n",
    "        try:\n",
    "            scores = self.rouge.get_scores(generated_summary, reference_summary)\n",
    "            return scores\n",
    "        except Exception as e:\n",
    "            print(f\"ROUGE evaluation error: {e}\")\n",
    "            return None\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        hindi_data = pd.read_csv(\"test.csv\", encoding=\"utf-8\", nrows=700)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Dataset not found.\")\n",
    "        return\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"The CSV file is empty.\")\n",
    "        return\n",
    "\n",
    "    hindi_data.dropna(subset=[\"summary\"], inplace=True)\n",
    "    hindi_data = hindi_data[hindi_data[\"article\"].str.split().str.len() >= 200]\n",
    "\n",
    "    summarizer = HindiTextSummarizer(summary_length=100)\n",
    "    similarity_measures = [\"common_tokens\", \"cosine\", \"jaccard\"]\n",
    "    document_counts = [30, 50, 100, 200, 300, 500]\n",
    "\n",
    "    compilation_results = []\n",
    "\n",
    "    for count in document_counts:\n",
    "        print(f\"\\nProcessing {count} documents...\")\n",
    "        subset = hindi_data.head(count)\n",
    "        results = []\n",
    "\n",
    "        for _, row in tqdm(subset.iterrows(), total=len(subset), desc=f\"Docs: {count}\"):\n",
    "            article, summary = row[\"article\"], row[\"summary\"]\n",
    "            if pd.isna(article) or pd.isna(summary):\n",
    "                continue\n",
    "\n",
    "            for measure in similarity_measures:\n",
    "                generated_summary = summarizer.generate_summary(article, similarity_measure=measure)\n",
    "                rouge_scores = summarizer.evaluate_summary(generated_summary, summary)\n",
    "                if rouge_scores:\n",
    "                    # Extract all metrics (precision, recall, f1) for each ROUGE score\n",
    "                    rouge1 = rouge_scores[0]['rouge-1']\n",
    "                    rouge2 = rouge_scores[0]['rouge-2']\n",
    "                    rougeL = rouge_scores[0]['rouge-l']\n",
    "                    \n",
    "                    results.append({\n",
    "                        \"Similarity Measure\": measure,\n",
    "                        \"ROUGE-1 Precision\": rouge1['p'],\n",
    "                        \"ROUGE-1 Recall\": rouge1['r'],\n",
    "                        \"ROUGE-1 F1\": rouge1['f'],\n",
    "                        \"ROUGE-2 Precision\": rouge2['p'],\n",
    "                        \"ROUGE-2 Recall\": rouge2['r'],\n",
    "                        \"ROUGE-2 F1\": rouge2['f'],\n",
    "                        \"ROUGE-L Precision\": rougeL['p'],\n",
    "                        \"ROUGE-L Recall\": rougeL['r'],\n",
    "                        \"ROUGE-L F1\": rougeL['f']\n",
    "                    })\n",
    "\n",
    "        result_df = pd.DataFrame(results)\n",
    "        if result_df.empty:\n",
    "            continue\n",
    "\n",
    "        avg_scores = result_df.groupby(\"Similarity Measure\").mean().reset_index()\n",
    "        avg_scores[\"Document Count\"] = count\n",
    "        compilation_results.append(avg_scores)\n",
    "\n",
    "    final_table = pd.concat(compilation_results, ignore_index=True)\n",
    "    # Reorder columns for better readability\n",
    "    columns_order = [\"Document Count\", \"Similarity Measure\"] + \\\n",
    "                    [f\"ROUGE-1 {m}\" for m in [\"Precision\", \"Recall\", \"F1\"]] + \\\n",
    "                    [f\"ROUGE-2 {m}\" for m in [\"Precision\", \"Recall\", \"F1\"]] + \\\n",
    "                    [f\"ROUGE-L {m}\" for m in [\"Precision\", \"Recall\", \"F1\"]]\n",
    "    final_table = final_table[columns_order]\n",
    "\n",
    "    print(\"\\nCompilation Table of Average ROUGE Scores (Precision, Recall, F1):\\n\")\n",
    "    print(final_table.to_string(index=False, float_format=\"%.3f\"))\n",
    "\n",
    "    final_table.to_csv(\"compiled_summary_rouge_scores_detailed.csv\", index=False)\n",
    "    print(\"\\nDetailed results exported to compiled_summary_rouge_scores_detailed.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23e1dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
